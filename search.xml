<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Notes of Learning Concurrent Programming in Scala</title>
      <link href="/notes/scala-concurrency/"/>
      <url>/notes/scala-concurrency/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://concurrent-programming-in-scala.github.io/" target="_blank" rel="noopener">Learning Concurrent Programming in Scala</a></p><a id="more"></a><h2 id="basic">basic</h2><h3 id="processes-and-threads">processes and threads</h3><ul><li>1 : 1</li><li>deterministic / nondeterministic</li></ul><h4 id="thread-states">thread states</h4><ul><li>new</li><li>runnable</li><li>running</li><li>waiting</li><li>terminated</li></ul><h3 id="monitors-and-synchronization">monitors and synchronization</h3><ul><li>monitor locks aka intrinsic locks</li><li>synchronized statements</li><li>wait and notify</li><li>deadlocks</li><li>guarded blocks</li><li>interrupting threads and graceful shutdown</li></ul><h3 id="lock-optimizations"><a href="https://blog.wqlin.me/2017/07/08/java-synchronized-keyword/" target="_blank" rel="noopener">lock optimizations</a></h3><ul><li>adaptive spinning</li><li>lock elimination</li><li>lock coarsening</li><li>lightweight locking</li><li>biased locking</li></ul><h3 id="volatile-variables">volatile variables</h3><ul><li>reordering</li><li>visible</li></ul><h3 id="memory-model"><a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html" target="_blank" rel="noopener">memory model</a></h3><ul><li>sequential consistency</li><li>happens before</li></ul><h3 id="immutable-objects">immutable objects</h3><ul><li>final fields</li></ul><h2 id="traditional-building-blocks">traditional building blocks</h2><h3 id="thread-pools">thread pools</h3><ul><li>Executor</li><li>ExecutorService</li><li>ExecutionContext</li><li><p>ForkJoinPool</p></li><li><p>starvation</p></li></ul><h3 id="atomic-variables">atomic variables</h3><ul><li>compare and swap (cas)</li><li>linearizable and atomic</li><li>lock-free programming</li><li>implementing locks explicitly</li><li>the aba problem <br>(store immutable values inside the atomic references)</li></ul><h3 id="lazy-values">lazy values</h3><ul><li>double-checked locking idiom <br>(never invoke blocking operations inside lazy value initialization expressions) <br>(never call synchronized on publicly available objects; always use a dedicated, private dummy object for synchronization)</li></ul><h3 id="concurrent-collections">concurrent collections</h3><h4 id="concurrent-queues">concurrent queues</h4><ul><li>BlockingQueue</li><li>ArrayBlockingQueue (bounded)</li><li>LinkedBlockingQueue (unbounded)</li></ul><table><thead><tr class="header"><th align="left">operation</th><th align="left">enqueue</th><th align="left">dequeue</th><th align="left">inspect</th></tr></thead><tbody><tr class="odd"><td align="left">exception</td><td align="left">add</td><td align="left">remove</td><td align="left">element</td></tr><tr class="even"><td align="left">special value</td><td align="left">offer</td><td align="left">poll</td><td align="left">peek</td></tr><tr class="odd"><td align="left">timed</td><td align="left">offer</td><td align="left">poll</td><td align="left"> </td></tr><tr class="even"><td align="left">blocking</td><td align="left">put</td><td align="left">take</td><td align="left"> </td></tr></tbody></table><ul><li>weakly consistent iterators <br>(use iterators on concurrent data structures only when you can ensure that no other thread will modify the data structure from the point where the iterator was created until the point where the iterator's hasnext method returns false)</li></ul><h4 id="concurrent-maps">concurrent maps</h4><ul><li>concurrent Map</li><li>ConcurrentHashMap asScala</li><li>ConcurrentSkipListSet asScala</li></ul><h5 id="complex-linearizable-methods">complex linearizable methods</h5><ul><li>putIfAbsent</li><li>remove/2</li><li>replace/2</li><li>replace/3 <br>(equivalent on 'equals method')</li></ul><h5 id="linearizable-methods">linearizable methods</h5><ul><li>+=</li><li>-=</li><li>put</li><li>update</li><li>get</li><li>apply</li><li>remove <br>(avoid using the null value as a key or a value in a concurrent data structure)</li></ul><h4 id="concurrent-traversals">concurrent traversals</h4><ul><li><a href="https://www.scala-lang.org/api/current/scala/collection/concurrent/TrieMap.html" target="_blank" rel="noopener">TrieMap</a> <br>(use TrieMap if you require consistent iterators and ConcurrentHashMap when the get and apply operations are the bottlenecks in your program)</li></ul><h3 id="processes">processes</h3><ul><li><a href="https://www.scala-lang.org/api/current/scala/sys/process/" target="_blank" rel="noopener">scala.sys.process</a></li></ul><h2 id="asynchronous">asynchronous</h2><h3 id="futures"><a href="https://docs.scala-lang.org/overviews/core/futures.html" target="_blank" rel="noopener">futures</a></h3><ul><li>apply</li><li>foreach</li><li>onComplete</li><li>failed</li><li>map</li><li>flatMap</li></ul><h3 id="promises">promises</h3><ul><li>apply</li><li>success</li><li>failure</li><li>complete</li><li><p>tryComplete</p></li><li><p>cancellation</p></li></ul><h3 id="await">await</h3><ul><li>ready</li><li>result</li><li>blocking (BlockContext)</li></ul><h3 id="async">async</h3><ul><li><a href="https://github.com/scala/scala-async" target="_blank" rel="noopener">scala async library</a></li></ul><h2 id="data-parallel-collections"><a href="https://docs.scala-lang.org/overviews/parallel-collections/overview.html" target="_blank" rel="noopener">data parallel collections</a></h2><ul><li>trivially parallelizable</li><li>modified exclusive shared invalid (mesi)</li><li>resource contention</li><li>memory contention <br>(writing to the same memory location with proper synchronization leads to performance bottlenecks and contention; avoid this in data-parallel operations)</li></ul><h3 id="hierarchy">hierarchy</h3><ul><li>GenTraversable</li><li>GenIterable<ul><li>ParIterable</li><li>ParSeq</li><li>ParMap</li><li>ParSet</li></ul></li></ul><h3 id="parallelism-level">parallelism level</h3><ul><li>TaskSupport</li></ul><h3 id="measuring-performance">measuring performance</h3><ul><li>interpreted mode</li><li>steady state</li></ul><h3 id="caveats">caveats</h3><h4 id="parallelizable-collections">parallelizable collections</h4><ul><li>Range</li><li>Vector</li><li>HashMap</li><li>HashSet</li><li>mutable Array</li><li>mutable ArrayBuffer</li><li>mutable HashMap</li><li>mutable HashSet</li><li>concurrent TrieMap</li></ul><h4 id="parallelizable-operations">parallelizable operations</h4><ul><li>fold</li><li>reduce</li><li>aggregate</li></ul><h4 id="non-parallelizable-operations">non-parallelizable operations</h4><ul><li>foldLeft</li><li>foldRight</li><li>reduceLeft</li><li>reduceRight</li><li>reduceLeftOption</li><li>reduceRightOption</li><li>scanLeft</li><li>scanRight</li><li>toList</li><li>toStream</li></ul><h3 id="side-effects">side effects</h3><p>(to avoid the need for synchronization and ensure better scalability, favor declarative-style parallel operations instead of the side effects in parallel for loops)</p><h3 id="nondeterministic-parallel-operations">nondeterministic parallel operations</h3><ul><li>find</li></ul><h3 id="associative-operators">associative operators</h3><ul><li>monoid</li></ul><h3 id="implementing-custom-parallel-collections">implementing custom parallel collections</h3><ul><li>immutable ParSeq</li><li>Splitter</li><li>IterableSplitter</li><li>SeqSplitter</li><li>Combiner</li></ul><h2 id="reactive-extensions">reactive extensions</h2><h3 id="observables">observables</h3><ul><li>cold observables</li><li>hot observables</li></ul><h4 id="constructors">constructors</h4><ul><li>from</li><li>just</li><li>timer</li><li>interval</li><li>error</li><li>create</li><li>apply</li></ul><h4 id="transformers">transformers</h4><ul><li>map</li><li>filter</li><li>take</li><li>concat</li><li>flatten</li><li>flatMap</li><li>timeout</li><li>merge</li><li>retry</li><li>repeat</li><li>scan</li><li>onErrorReturn</li><li>onErrorResumeNext</li><li>observeOn</li></ul><h4 id="accessors">accessors</h4><ul><li>subscribe</li></ul><h3 id="observers">observers</h3><ul><li>onNext</li><li>onError</li><li>onCompleted</li></ul><h3 id="subscriptions">subscriptions</h3><ul><li>apply</li><li>unsubscribe</li></ul><h3 id="schedulers">schedulers</h3><ul><li>ComputationScheduler</li><li>IOScheduler</li><li>NewThreadScheduler</li><li>Schedulers from</li></ul><h3 id="subjects">subjects</h3><ul><li>Subject</li><li>ReplaySubject</li><li>BehaviorSubject</li><li><p>AsyncSubject</p></li><li><p>top-down programming style</p></li></ul><h2 id="software-transactional-memory">software transactional memory</h2><ul><li>memory transactions</li><li>isolation</li><li>composability</li></ul><h3 id="interaction-between-transactions-and-side-effects">interaction between transactions and side effects</h3><p>(only use the transactional context within the thread that started the transaction)</p><p>(avoid external side effects inside the transactions, as the transactions can be re-executed multiple times)</p><p>(use the Txn's afterCommit and afterRollback methods to perform side-effecting operations in the transactions without the danger of executing them multiple times)</p><h3 id="single-operation-transactions">single-operation transactions</h3><p>(use single-operation transactions for single read, write, and cas-like operations in order to avoid the syntactic boilerplate associated with the atomic blocks)</p><h3 id="nesting-transactions">nesting transactions</h3><p>(nested atomic blocks result in a transaction that starts when the top-level atomic block starts, and can commit only after the top-level atomic block completes)</p><h3 id="transactions-and-exceptions">transactions and exceptions</h3><p>(when an exception is thrown inside a transaction, the transaction is rolled back and the exception is rethrown at the point where the top-level atomic block started)</p><h3 id="retrying-transactions">retrying transactions</h3><p>(avoid long-running transactions whenever possible. never execute an infinite loop inside a transaction, as it can cause deadlocks)</p><p>(use the retry statement to block the transaction until a specific condition is fulfilled, and retry the transaction automatically once its read set changes)</p><h3 id="retrying-with-timeouts">retrying with timeouts</h3><p>(when a timeout represents exceptional program behavior, use the withRetryTimeout method to set the timeout duration in the transaction; when the transaction proceeds normally after a timeout, use the retryFor method)</p><h3 id="transactional-collections">transactional collections</h3><ul><li>transaction-local variable</li><li>transactional arrays</li><li>transactional maps</li></ul><h2 id="actor">actor</h2><h3 id="ops-on-actor-ref">ops on actor-ref</h3><ul><li><code>!</code> tell / bang</li><li>forward</li><li><code>?</code> ask (pattern)</li><li>pipeTo (pattern)</li></ul><h3 id="ops-on-system">ops on system</h3><ul><li>actorOf</li><li>actorSelection</li><li>terminate</li></ul><h3 id="ops-on-context">ops on context</h3><ul><li>system</li><li>props</li><li>self</li><li>sender</li><li>children</li><li>parent</li><li>become</li><li>watch</li><li>stop</li></ul><h3 id="messages">messages</h3><ul><li>Identify</li><li>ActorIdentity</li><li>Kill</li><li>PoisonPill</li><li>Terminated</li></ul><p>unhandled</p><h3 id="life-cycles">life cycles</h3><ul><li>preStart</li><li>postStop</li><li>preRestart</li><li>postRestart</li></ul><h3 id="supervisions">supervisions</h3><ul><li>Restart</li><li>Resume</li><li>Stop</li><li>Escalate</li></ul><h4 id="strategies">strategies</h4><ul><li>OneForOne</li><li>AllForOne</li></ul><h2 id="concurrency-in-practice">concurrency in practice</h2><p>(there is no one-size-fits-all technology, use your own best judgment when deciding which concurrency framework to use for a specific programming task)</p><h3 id="utilities">utilities</h3><ul><li>jvisualvm</li><li>scalameter</li></ul><h2 id="references">references</h2><ul><li><a href="https://hongjiev.github.io/2017/07/05/Threads-And-Locks-md/" target="_blank" rel="noopener">https://hongjiev.github.io/2017/07/05/Threads-And-Locks-md/</a></li><li><a href="https://docs.oracle.com/javase/specs/" target="_blank" rel="noopener">https://docs.oracle.com/javase/specs/</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of K-Means</title>
      <link href="/notes/ml-k-means/"/>
      <url>/notes/ml-k-means/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/lecture/czmip/unsupervised-learning-introduction" target="_blank" rel="noopener">K-Means</a></p><a id="more"></a><h2 id="input">Input</h2><ul><li><span class="math">\(K\)</span> (number of clusters)</li><li>Training set <span class="math">\(\{ x^{(1)}, x^{(2)} \dots x^{(m)} \}\)</span></li></ul><h2 id="algorithm">Algorithm</h2><p>Randomly initialize <span class="math">\(K\)</span> cluster centroids <span class="math">\(\mu_1, \mu_2 \dots \mu_K \in \mathbb{R}^n\)</span></p><p>Repeat <br><span class="math">\(\;\;\)</span> for <span class="math">\(i\)</span> = <span class="math">\(1\)</span> to <span class="math">\(m\)</span> <br><span class="math">\(\;\;\;\;\)</span> <span class="math">\(c^{(i)}\)</span> := index (from <span class="math">\(1\)</span> to <span class="math">\(K\)</span>) of cluster centroid closest to <span class="math">\(x^{(i)}\)</span> <br><span class="math">\(\;\;\)</span> for <span class="math">\(k\)</span> = <span class="math">\(1\)</span> to <span class="math">\(K\)</span> <br><span class="math">\(\;\;\;\;\)</span> <span class="math">\(\mu_k\)</span> := average (mean) of points assigned to cluster <span class="math">\(k\)</span></p><p>Run 100 times, pick clustering that gave lowest cost <span class="math">\(J(c^{(1)} \dots c^{(m)}, \mu_1 \dots \mu_K)\)</span></p><h2 id="optimization-objective">Optimization Objective</h2><p><span class="math">\(\displaystyle J(c^{(1)} \dots c^{(m)}, \mu_1 \dots \mu_K) = \frac{1}{m} \sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}|| ^ 2\)</span></p><!--$\displaystyle \min_{\underset{c^{(1)} \dots c^{(m)}}{\mu_1 \dots \mu_K}}J(c^{(1)} \dots c^{(m)}, \mu_1 \dots \mu_K)$--><h2 id="random-initialization">Random Initialization</h2><ul><li>Should have <span class="math">\(K &lt; m\)</span></li><li>Randomly pick <span class="math">\(K\)</span> training examples</li><li>Set <span class="math">\(\mu_1 \dots \mu_K\)</span> equal to these <span class="math">\(K\)</span> examples</li></ul><h2 id="choosing-the-number-of-clusters">Choosing the Number of Clusters</h2><p>Sometimes, you are running K-means to get clusters to use for some later or downstream purpose Evaluate K-means based on a metric for how well it performs for that later purpose</p><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/hFF7A/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/hFF7A/lecture-slides</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Diagnosing Learning Algorithm</title>
      <link href="/notes/ml-diagnosing/"/>
      <url>/notes/ml-diagnosing/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/supplement/aFpD3/evaluating-a-hypothesis" target="_blank" rel="noopener">Diagnosing Learning Algorithm</a></p><a id="more"></a><h2 id="datasets">Datasets</h2><ul><li>Training set: 60%</li><li>Cross validation set: 20%</li><li>Test set: 20%</li></ul><p><span class="math">\(\displaystyle J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}} \left( h_\theta(x_{test}^{(i)}) - y_{test}^{(i)} \right) ^ 2\)</span></p><h2 id="bias-and-variance">Bias and Variance</h2><ul><li>High bias (underfit): <span class="math">\(J_{train}(\theta)\)</span> will be high and <span class="math">\(J_{CV}(\theta) \approx J_{train}(\theta)\)</span></li><li>High variance (overfit): <span class="math">\(J_{train}(\theta)\)</span> will be low and <span class="math">\(J_{CV}(\theta) \gg J_{train}(\theta)\)</span></li></ul><h2 id="learning-curves">Learning Curves</h2><p>plot(1:m, <span class="math">\(J_{train}(\theta)\)</span>, 1:m, <span class="math">\(J_{CV}(\theta)\)</span>)</p><ul><li>Experiencing high bias:</li><li>Low training set size: <span class="math">\(J_{train}(\theta)\)</span> will be low and <span class="math">\(J_{CV}(\theta)\)</span> will be high</li><li>Large training set size: both <span class="math">\(J_{train}(\theta)\)</span> and <span class="math">\(J_{CV}(\theta)\)</span> will be high with <span class="math">\(J_{train}(\theta) \approx J_{CV}(\theta)\)</span></li><li>Experiencing high variance:</li><li>Low training set size: <span class="math">\(J_{train}(\theta)\)</span> will be low and <span class="math">\(J_{CV}(\theta)\)</span> will be high</li><li>Large training set size: <span class="math">\(J_{train}(\theta)\)</span> increases with training set size and <span class="math">\(J_{CV}(\theta)\)</span> continues to decrease without leveling off, also <span class="math">\(J_{train}(\theta) &lt; J_{CV}(\theta)\)</span> but the difference between them remains significant</li></ul><h2 id="deciding-what-to-do-next">Deciding What to Do Next</h2><ul><li>Getting more training examples: Fixes high variance</li><li>Trying smaller sets of features: Fixes high variance</li><li>Adding features: Fixes high bias</li><li>Adding polynomial features: Fixes high bias</li><li>Decreasing <span class="math">\(\lambda\)</span>: Fixes high bias</li><li>Increasing <span class="math">\(\lambda\)</span>: Fixes high variance</li></ul><h2 id="error-analysis">Error Analysis</h2><ul><li>Start with a simple algorithm, implement it quickly, and test it early on your cross validation data</li><li>Plot learning curves to decide if more data, more features, etc. are likely to help</li><li>Manually examine errors on examples in cross validation set and try to spot a trend where most of errors were made</li></ul><h2 id="precision-and-recall">Precision and Recall</h2><table><thead><tr class="header"><th align="left"></th><th align="left">actual 1</th><th align="left">actual 0</th></tr></thead><tbody><tr class="odd"><td align="left"><strong>predicted 1</strong></td><td align="left">true positive</td><td align="left">false positive</td></tr><tr class="even"><td align="left"><strong>predicted 0</strong></td><td align="left">false negative</td><td align="left">true negative</td></tr></tbody></table><p><span class="math">\(\text{accuracy} = \dfrac{tp + tn}{tp + tn + fp + fn}\)</span></p><p><span class="math">\(\text{precision} = \dfrac{tp}{tp + fp}\)</span></p><p><span class="math">\(\text{recall} = \dfrac{tp}{tp + fn}\)</span></p><p><span class="math">\(F_1 = \dfrac{2 \cdot precision \cdot recall}{precision + recall}\)</span></p><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/7BHrF/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/7BHrF/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/gFC7y/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/gFC7y/lecture-slides</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of ES Modules and CommonJS Modules</title>
      <link href="/notes/es-modules-and-commonjs-modules/"/>
      <url>/notes/es-modules-and-commonjs-modules/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="http://exploringjs.com/es6/ch_modules.html" target="_blank" rel="noopener">ES Modules</a> and <a href="http://wiki.commonjs.org/wiki/Modules/1.1" target="_blank" rel="noopener">CommonJS Modules</a></p><a id="more"></a><h2 id="node">Node</h2><blockquote><p>Node version: v8.5.0</p></blockquote><ul><li><a href="https://nodejs.org/api/modules.html" target="_blank" rel="noopener">Node Modules</a></li><li><a href="https://nodejs.org/api/esm.html" target="_blank" rel="noopener">Node ECMAScript Modules</a></li><li><p><a href="https://github.com/nodejs/node-eps/blob/master/002-es-modules.md" target="_blank" rel="noopener">Node EP for ES Modules</a></p></li><li>The <code>--experimental-modules</code> flag can be used to enable features for loading ES modules</li><li>ES modules imports will be loaded asynchronously</li><li>ES modules files will use <code>.mjs</code> extension</li><li><p>The <code>.mjs</code> file extension will not be loadable via <code>require()</code></p></li></ul><h3 id="es-consuming-commonjs">ES consuming CommonJS</h3><p><code>cjs.js</code>:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123; <span class="attr">v</span>: <span class="number">1</span> &#125;</span><br><span class="line"><span class="built_in">module</span>.exports.f = <span class="function"><span class="params">()</span> =&gt;</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><p><code>esm.mjs</code>:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> assert <span class="keyword">from</span> <span class="string">'assert'</span></span><br><span class="line"><span class="keyword">import</span> cjs <span class="keyword">from</span> <span class="string">'./cjs'</span></span><br><span class="line"></span><br><span class="line">assert.equal(cjs.v, <span class="number">1</span>)</span><br><span class="line">assert.equal(cjs.f(), <span class="number">2</span>)</span><br></pre></td></tr></table></figure><!--### CommonJS consuming ES--><h3 id="unsupported-features">Unsupported Features</h3><ul><li><code>import()</code></li><li><code>import.meta</code></li><li>Loader Hooks</li></ul><h2 id="babel-transform-plugin">Babel Transform Plugin</h2><blockquote><p>Babel version: v6.26.0 Babel plugin <a href="http://babeljs.io/docs/plugins/transform-es2015-modules-commonjs" target="_blank" rel="noopener">transform-es2015-modules-commonjs</a></p></blockquote><h3 id="export">Export</h3><p>In:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123; <span class="attr">v</span>: <span class="number">1</span> &#125;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> f = <span class="function"><span class="params">()</span> =&gt;</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Object</span>.defineProperty(exports, <span class="string">'__esModule'</span>, &#123; <span class="attr">value</span>: <span class="literal">true</span> &#125;)</span><br><span class="line">exports.default = &#123; <span class="attr">v</span>: <span class="number">1</span> &#125;</span><br><span class="line"><span class="keyword">const</span> f = exports.f = <span class="function"><span class="params">()</span> =&gt;</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="import">Import</h3><p>In:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> x <span class="keyword">from</span> <span class="string">'x'</span></span><br><span class="line"></span><br><span class="line">x.v</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> __x = <span class="built_in">require</span>(<span class="string">'x'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> _x = _interopRequireDefault(__x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> x = _x.default</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">_interopRequireDefault</span>(<span class="params">o</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> o &amp;&amp; o.__esModule ? o : &#123; <span class="attr">default</span>: o &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">x.v</span><br></pre></td></tr></table></figure><hr><p>In:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> &#123; y &#125; <span class="keyword">from</span> <span class="string">'y'</span></span><br><span class="line"></span><br><span class="line">y.v</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> _y = <span class="built_in">require</span>(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> y = _y.y</span><br><span class="line"></span><br><span class="line">y.v</span><br></pre></td></tr></table></figure><hr><p>In:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> z <span class="keyword">from</span> <span class="string">'z'</span></span><br><span class="line"></span><br><span class="line">z.v</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> _z = <span class="built_in">require</span>(<span class="string">'z'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> z = _interopRequireWildcard(_z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">_interopRequireWildcard</span>(<span class="params">o</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (o &amp;&amp; o.__esModule) &#123;</span><br><span class="line">    <span class="keyword">return</span> o</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> m = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> (o != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">in</span> o) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">Object</span>.prototype.hasOwnProperty.call(o, key))</span><br><span class="line">          m[key] = o[key]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    m.default = o</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">z.v</span><br></pre></td></tr></table></figure><!--## Typescript--><hr>]]></content>
      
      
        <tags>
            
            <tag> ECMAScript </tag>
            
            <tag> Node </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Neural Networks</title>
      <link href="/notes/ml-neural-networks/"/>
      <url>/notes/ml-neural-networks/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/supplement/Bln5m/model-representation-i" target="_blank" rel="noopener">Neural Networks</a></p><a id="more"></a><h2 id="model-representation">Model Representation</h2><h3 id="neuron">Neuron</h3><img src="/notes/ml-neural-networks/neuron.png"><h3 id="neural-network">Neural Network</h3><img src="/notes/ml-neural-networks/neural-network.png"><p><span></span></p><!--Layer 1: input layerLayer 2: hidden layerLayer 3: output layer--><p><span class="math">\(\begin{bmatrix} x_1 \newline x_2 \newline x_3 \newline \end{bmatrix} \rightarrow \begin{bmatrix} a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix} \rightarrow h_\theta(x)\)</span></p><!--$\begin{align}a_1^{(2)} & = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newlinea_2^{(2)} & = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newlinea_3^{(2)} & = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newlinea_1^{(3)} & = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline\end{align}$--><p><span class="math">\(a^{(l)} = g(\Theta^{(l-1)} a^{(l-1)})\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(l\)</span>: index of layer <br><span class="math">\(\;\;\)</span> <span class="math">\(a^{(l)}\)</span>: &quot;activation&quot; in layer <span class="math">\(l\)</span> <br><span class="math">\(\;\;\)</span> <span class="math">\(a_i^{(l)}\)</span>: &quot;activation&quot; of unit <span class="math">\(i\)</span> in layer <span class="math">\(l\)</span> <br><span class="math">\(\;\;\)</span> <span class="math">\(a_0^{(l)} = 1\)</span>: bias units <br><span class="math">\(\;\;\)</span> <span class="math">\(a^{(1)} = x\)</span>: input layer <br><span class="math">\(\;\;\)</span> <span class="math">\(\Theta^{(l)} \in \mathbb{R} ^ {s_{l+1} \times (s_l + 1)}\)</span>: matrix of weights <br><span class="math">\(\;\;\)</span> <span class="math">\(s_l\)</span>: number of units in layer <span class="math">\(l\)</span></p><h2 id="multiclass-classification">Multiclass Classification</h2><img src="/notes/ml-neural-networks/neural-network-multiclass.png"><p><span></span></p><p><span class="math">\(y \in \lbrace \begin{bmatrix} 1 \newline 0 \newline \vdots \newline 0 \end{bmatrix} , \begin{bmatrix} 0 \newline 1 \newline \vdots \newline 0 \end{bmatrix} \dots \begin{bmatrix} 0 \newline 0 \newline \vdots \newline 1 \end{bmatrix} \rbrace\)</span></p><p><span class="math">\(h_\Theta(x) \in \mathbb{R} ^ {K}\)</span></p><p><span class="math">\(\mathrm{prediction} = \max_k(h_\Theta(x)_k)\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(K\)</span>: number of classes</p><h2 id="cost-function">Cost Function</h2><p><span class="math">\(\displaystyle \begin{gather} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[ y^{(i)}_k \log((h_\Theta(x^{(i)}))_k) + (1 - y^{(i)}_k) \log(1 - (h_\Theta(x^{(i)}))_k) \right] + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\Theta_{j,i}^{(l)})^2 \end{gather}\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(L\)</span>: total number of layers <br><span class="math">\(\;\;\)</span> <span class="math">\(K\)</span>: number of classes / output units <br><span class="math">\(\;\;\)</span> <span class="math">\(s_l\)</span>: number of units in layer <span class="math">\(l\)</span></p><blockquote><p>The double sum simply adds up the logistic regression costs calculated for each cell in the output layer The triple sum simply adds up the squares of all the individual <span class="math">\(\Theta\)</span>s in the entire network</p></blockquote><h2 id="back-propagation">Back Propagation</h2><blockquote><p>Goal: <span class="math">\(minimize _\Theta J(\Theta)\)</span></p></blockquote><h3 id="computing-partial-derivatives">Computing Partial Derivatives</h3><!--$\dfrac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta)$--><p>TODO</p><h3 id="unrolling-parameters">Unrolling Parameters</h3><p>For example, <span class="math">\(\Theta^{(1)} \in \mathbb{R} ^ {10 \times 11}\)</span>, <span class="math">\(\Theta^{(2)} \in \mathbb{R} ^ {10 \times 11}\)</span>, <span class="math">\(\Theta^{(3)} \in \mathbb{R} ^ {1 \times 11}\)</span></p><p>Unroll:</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unrolledTheta <span class="built_in">=</span> [Theta1(:); Theta2(:); Theta3(:)]</span><br></pre></td></tr></table></figure><p>Reshape:</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 <span class="built_in">=</span> reshape(unrolledTheta(<span class="number">1</span>:<span class="number">110</span>), <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line">Theta2 <span class="built_in">=</span> reshape(unrolledTheta(<span class="number">111</span>:<span class="number">220</span>), <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line">Theta3 <span class="built_in">=</span> reshape(unrolledTheta(<span class="number">221</span>:<span class="number">231</span>), <span class="number">1</span>, <span class="number">11</span>)</span><br></pre></td></tr></table></figure><h3 id="gradient-checking">Gradient Checking</h3><p>Check that   <span class="math">\(\displaystyle \frac{\partial}{\partial \Theta} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2 \epsilon}\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\epsilon\)</span>: a small value, usually set to <span class="math">\(10^{-4}\)</span></p><!--$\displaystyle\frac{\partial}{\partial \Theta_i} J(\Theta) \approx\frac{J(\Theta_1 \dots \Theta_i + \epsilon \dots \Theta_n) - J(\Theta_1 \dots \Theta_i - \epsilon \dots \Theta_n)}{2 \epsilon}$--><h3 id="random-initialization">Random Initialization</h3><!--> Initializing all theta weights to zero does not work with neural networks> When backpropagate, all nodes will update to the same value repeatedly--><p>Initialize each <span class="math">\(\Theta_{ij}^{(l)}\)</span> to a random value in <span class="math">\([-\epsilon, \epsilon]\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\epsilon = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}\)</span> <br><span class="math">\(\;\;\)</span> <span class="math">\(L_{in} = s_l\)</span> <br><span class="math">\(\;\;\)</span> <span class="math">\(L_{out} = s_{l+1}\)</span></p><h3 id="network-architecture">Network Architecture</h3><ul><li>Number of input units = dimension of features <span class="math">\(x^{(i)}\)</span></li><li>Number of output units = number of classes</li><li>Number of hidden units per layer = usually more the better (cost of computation increases with more hidden units)</li><li>Number of hidden layers = defaults 1 (if ＞ 1, it is recommended to have same number of units in every hidden layer)</li></ul><h3 id="summary">Summary</h3><ol style="list-style-type: decimal"><li>Randomly initialize weights</li><li>Implement forward propagation to get <span class="math">\(h_\Theta(x^{(i)})\)</span> for any <span class="math">\(x^{(i)}\)</span></li><li>Implement code to compute cost function <span class="math">\(J(\Theta)\)</span></li><li>Implement back propagation to compute partial derivatives</li><li>Use gradient checking to confirm that back propagation works (then disable gradient checking)</li><li>Use gradient descent or advanced optimization method with back propagation to minimize <span class="math">\(J(\Theta)\)</span> as a function of <span class="math">\(\Theta\)</span></li></ol><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/jtFHI/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/jtFHI/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/FklyY/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/FklyY/lecture-slides</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Logistic Regression</title>
      <link href="/notes/ml-logistic-regression/"/>
      <url>/notes/ml-logistic-regression/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/supplement/fDCQp/classification" target="_blank" rel="noopener">Logistic Regression</a></p><a id="more"></a><h2 id="hypothesis-representation">Hypothesis Representation</h2><p><span class="math">\(h_\theta(x) = g(\theta^T x)\)</span></p><h3 id="sigmoid-function">Sigmoid Function</h3><p><span class="math">\(g(z) = \dfrac{1}{1 + e^{-z}}\)</span>   <span class="math">\(\in (0, 1)\)</span></p><h3 id="probability">Probability</h3><p><span class="math">\(P(y = 1 \mid x; \theta) = h_\theta(x)\)</span></p><p><span class="math">\(P(y = 0 \mid x; \theta) = 1 - h_\theta(x)\)</span></p><h2 id="decision-boundary">Decision Boundary</h2><blockquote><p>The line that separates the area where y = 0 and where y = 1</p></blockquote><p><span class="math">\(\theta^T x \geq 0\)</span>   <span class="math">\(\to\)</span>   <span class="math">\(h_\theta(x) \geq 0.5\)</span>   <span class="math">\(\to\)</span>   <span class="math">\(y = 1\)</span></p><p><span class="math">\(\theta^T x &lt; 0\)</span>   <span class="math">\(\to\)</span>   <span class="math">\(h_\theta(x) &lt; 0.5\)</span>   <span class="math">\(\to\)</span>   <span class="math">\(y = 0\)</span></p><h2 id="cost-function">Cost Function</h2><p><span class="math">\(\displaystyle J(\theta) = \frac{1}{m} \sum_{i=1}^m \mathrm{cost}(h_\theta(x^{(i)}), y^{(i)})\)</span></p><p><span class="math">\(\mathrm{cost}(h_\theta(x), y) = - y \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x)) = \begin{cases} -\log(h_\theta(x))     &amp; \quad \text{if} \; y = 1 \newline -\log(1 - h_\theta(x)) &amp; \quad \text{if} \; y = 0 \newline \end{cases}\)</span></p><p><span class="math">\(s.t.\)</span>   <span class="math">\(\mathrm{cost}(h_\theta(x), y) = \begin{cases} 0          &amp; \quad \text{if} \; h_\theta(x) = y \newline \to \infty &amp; \quad \text{if} \; y = 1 \; \mathrm{and} \; h_\theta(x) \to 0 \newline \to \infty &amp; \quad \text{if} \; y = 0 \; \mathrm{and} \; h_\theta(x) \to 1 \newline \end{cases}\)</span></p><h3 id="vectorization">Vectorization</h3><p><span class="math">\(\displaystyle J(\theta) = \frac{1}{m} \left( - y^T \log(g(X \theta)) - (1 - y)^T \log(1 - g(X \theta)) \right)\)</span></p><h2 id="gradient-descent">Gradient Descent</h2><p>Repeat until convergence <br><span class="math">\(\;\;\)</span> <span class="math">\(\displaystyle \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \quad \text{for} \; j \gets 0 \dots n\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\alpha\)</span>: learning rate <br><span class="math">\(\;\)</span> <span class="math">\(\displaystyle \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \; x_j^{(i)}\)</span></p><p>(simultaneous update)</p><h3 id="vectorization-1">Vectorization</h3><p><span class="math">\(\displaystyle \theta := \theta - \alpha \frac{1}{m} X^T (g(X \theta) - y)\)</span></p><h2 id="multiclass-classification">Multiclass Classification</h2><blockquote><p>one-vs-all</p></blockquote><p><span class="math">\(y \in \lbrace 1 \dots k \rbrace\)</span></p><p><span class="math">\(h_\theta(x) \in \mathbb{R} ^ {K}\)</span></p><p><span class="math">\(h_\theta(x)_k = P(y = k \mid x; \theta)\)</span></p><p><span class="math">\(\mathrm{prediction} = \max_k(h_\theta(x)_k)\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(K\)</span>: number of classes</p><h2 id="regularization">Regularization</h2><p><span class="math">\(\displaystyle J(\theta) = \frac{1}{m} \sum_{i=1}^m \mathrm{cost}(h_\theta(x^{(i)}), y^{(i)}) + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\lambda\)</span>: regularization parameter</p><blockquote><p><span class="math">\(\displaystyle \sum_{j=1}^n \theta_j^2\)</span> excludes the bias term <span class="math">\(\theta_0\)</span></p></blockquote><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/QEYX8/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/QEYX8/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/CUz2O/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/CUz2O/lecture-slides</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Vim Operations</title>
      <link href="/notes/vim-operations/"/>
      <url>/notes/vim-operations/</url>
      <content type="html"><![CDATA[<p> </p><a id="more"></a><h2 id="help">help</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>:h</code> or <code>:help</code></td><td align="left">help</td></tr><tr class="even"><td align="left"><code>:h x</code></td><td align="left">help for command <code>x</code></td></tr><tr class="odd"><td align="left"><code>:h 'number'</code></td><td align="left">help for option <code>number</code></td></tr><tr class="even"><td align="left"><code>:h E37</code></td><td align="left">help for error message 'E37'</td></tr></tbody></table><hr><h2 id="modes">modes</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left">normal mode</td><td align="left"><code>&lt;esc&gt;</code></td></tr><tr class="even"><td align="left">insert mode</td><td align="left"><code>i</code></td></tr><tr class="odd"><td align="left">visual mode</td><td align="left"><code>v</code></td></tr></tbody></table><hr><h2 id="scroll">scroll</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>ctrl-d</code></td><td align="left">scroll downwards half a window</td></tr><tr class="even"><td align="left"><code>ctrl-u</code></td><td align="left">scroll upwards half a window</td></tr></tbody></table><h2 id="undo">undo</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>u</code></td><td align="left">undo</td></tr><tr class="even"><td align="left"><code>ctrl-r</code></td><td align="left">redo</td></tr></tbody></table><h2 id="motion">motion</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>h</code></td><td align="left">←</td></tr><tr class="even"><td align="left"><code>j</code></td><td align="left">↓</td></tr><tr class="odd"><td align="left"><code>k</code></td><td align="left">↑</td></tr><tr class="even"><td align="left"><code>l</code></td><td align="left">→</td></tr><tr class="odd"><td align="left"><code>w</code></td><td align="left">words forward</td></tr><tr class="even"><td align="left"><code>b</code></td><td align="left">words backward</td></tr><tr class="odd"><td align="left"><code>e</code></td><td align="left">to end of word</td></tr><tr class="even"><td align="left"><code>0</code></td><td align="left">to begin of line</td></tr><tr class="odd"><td align="left"><code>$</code></td><td align="left">to end of line</td></tr><tr class="even"><td align="left"><code>gg</code></td><td align="left">to first line</td></tr><tr class="odd"><td align="left"><code>G</code></td><td align="left">to last line</td></tr><tr class="even"><td align="left"><code>:5</code></td><td align="left">to 5th line</td></tr><tr class="odd"><td align="left"><code>%</code></td><td align="left">to matched pair</td></tr><tr class="even"><td align="left"><code>fx</code></td><td align="left">to next occurrence of char 'x'</td></tr><tr class="odd"><td align="left"><code>Fx</code></td><td align="left">to previous occurrence of char 'x'</td></tr><tr class="even"><td align="left"><code>ctrl-o</code></td><td align="left">to older position</td></tr><tr class="odd"><td align="left"><code>ctrl-i</code></td><td align="left">to newer position</td></tr></tbody></table><h2 id="insert">insert</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>i</code></td><td align="left">insert</td></tr><tr class="even"><td align="left"><code>I</code></td><td align="left">insert at begin of line</td></tr><tr class="odd"><td align="left"><code>a</code></td><td align="left">append</td></tr><tr class="even"><td align="left"><code>A</code></td><td align="left">append at end of line</td></tr><tr class="odd"><td align="left"><code>o</code></td><td align="left">new line below</td></tr><tr class="even"><td align="left"><code>O</code></td><td align="left">new line above</td></tr></tbody></table><h2 id="pattern">pattern</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>/pattern</code></td><td align="left">search forward</td></tr><tr class="even"><td align="left"><code>?pattern</code></td><td align="left">search backward</td></tr><tr class="odd"><td align="left"><code>*</code></td><td align="left">search forward for word</td></tr><tr class="even"><td align="left"><code>#</code></td><td align="left">search backward for word</td></tr><tr class="odd"><td align="left"><code>n</code></td><td align="left">next</td></tr><tr class="even"><td align="left"><code>N</code></td><td align="left">previous</td></tr><tr class="odd"><td align="left"><code>:noh</code></td><td align="left">stop highlighting</td></tr></tbody></table><h2 id="change">change</h2><h3 id="delete">delete</h3><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>x</code> or <code>dl</code></td><td align="left">delete char</td></tr><tr class="even"><td align="left"><code>dd</code></td><td align="left">delete line</td></tr><tr class="odd"><td align="left"><code>daw</code></td><td align="left">delete a word</td></tr><tr class="even"><td align="left"><code>d</code> in visual mode</td><td align="left">delete selected chars</td></tr><tr class="odd"><td align="left"><code>:1,5 d</code></td><td align="left">delete lines 1 - 5</td></tr></tbody></table><h3 id="copy">copy</h3><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>yy</code></td><td align="left">yank line</td></tr><tr class="even"><td align="left"><code>y</code> in visual mode</td><td align="left">yank selected chars</td></tr><tr class="odd"><td align="left"><code>:1,5 co 6</code></td><td align="left">copy lines 1 - 5 to below line 6</td></tr></tbody></table><h3 id="replace">replace</h3><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>rx</code></td><td align="left">replace char with 'x'</td></tr><tr class="even"><td align="left"><code>:s/old/new/</code></td><td align="left">substitute</td></tr><tr class="odd"><td align="left"><code>:%s/old/new/g</code></td><td align="left">substitute all</td></tr></tbody></table><h3 id="put">put</h3><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>p</code></td><td align="left">put</td></tr><tr class="even"><td align="left"><code>:1,5 m 6</code></td><td align="left">move lines 1 - 5 to below line 6</td></tr></tbody></table><h2 id="editing">editing</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>ZZ</code> or <code>:x</code></td><td align="left">write if modified and exit</td></tr><tr class="even"><td align="left"><code>ZQ</code> or <code>:q!</code></td><td align="left">quit without writing</td></tr><tr class="odd"><td align="left"><code>:w [file]</code></td><td align="left">write</td></tr><tr class="even"><td align="left"><code>:r [file]</code></td><td align="left">read and insert</td></tr><tr class="odd"><td align="left"><code>:e!</code></td><td align="left">re-edit</td></tr><tr class="even"><td align="left"><code>:f</code> or <code>ctrl-g</code></td><td align="left">print file status</td></tr><tr class="odd"><td align="left"><code>g ctrl-g</code></td><td align="left">print word count</td></tr></tbody></table><h2 id="windows">windows</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>:split [file]</code></td><td align="left">split window</td></tr><tr class="even"><td align="left"><code>:vsplit [file]</code></td><td align="left">split window vertically</td></tr><tr class="odd"><td align="left"><code>:close</code></td><td align="left">close window</td></tr><tr class="even"><td align="left"><code>:only</code></td><td align="left">close all other windows</td></tr><tr class="odd"><td align="left"><code>ctrl-w w</code></td><td align="left">move to next window</td></tr></tbody></table><h2 id="repeat">repeat</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>5x</code></td><td align="left">do <code>x</code> 5 times</td></tr></tbody></table><h2 id="macros">macros</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>qx ... q</code></td><td align="left">record macro named 'x'</td></tr><tr class="even"><td align="left"><code>@x</code></td><td align="left">execute macro named 'x'</td></tr></tbody></table><h2 id="shell">shell</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>:!ls</code></td><td align="left">execute <code>ls</code> with shell</td></tr></tbody></table><h2 id="auto-complete">auto-complete</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left"><code>ctrl-d</code></td><td align="left">list matched names</td></tr><tr class="even"><td align="left"><code>&lt;tab&gt;</code></td><td align="left">complete</td></tr></tbody></table><hr>]]></content>
      
      
        <tags>
            
            <tag> Vim </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Linear Algebra Review</title>
      <link href="/notes/ml-linear-algebra-review/"/>
      <url>/notes/ml-linear-algebra-review/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/supplement/Q6mSN/matrices-and-vectors" target="_blank" rel="noopener">Linear Algebra Review</a></p><a id="more"></a><h2 id="matrix">Matrix</h2><p><span class="math">\(\mathbb{R} ^ {3 \times 2}\)</span>:</p><p><span class="math">\[\begin{bmatrix}a &amp; b \newlinec &amp; d \newlinee &amp; f \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>; <span class="number">3</span>, <span class="number">4</span>; <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">[rows, columns] <span class="built_in">=</span> size(A)</span><br></pre></td></tr></table></figure><p><span class="math">\(A_{ij}\)</span>: element in the ith row and jth column of matrix <span class="math">\(A\)</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A(<span class="number">3</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="vector">Vector</h2><p><span class="math">\(\mathbb{R} ^ {3}\)</span>:</p><p><span class="math">\[\begin{bmatrix}x \newliney \newlinez \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v <span class="built_in">=</span> [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><span class="math">\(v_i\)</span>: element in the ith row of the vector <span class="math">\(v\)</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="addition">Addition</h2><p><span class="math">\[\begin{bmatrix}a &amp; b \newlinec &amp; d \newline\end{bmatrix}+\begin{bmatrix}w &amp; x \newliney &amp; z \newline\end{bmatrix}=\begin{bmatrix}a + w &amp; b + x \newlinec + y &amp; d + z \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>; <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">B <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>; <span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">A + B</span><br></pre></td></tr></table></figure><h2 id="scalar-multiplication">Scalar Multiplication</h2><p><span class="math">\[\begin{bmatrix}a &amp; b \newlinec &amp; d \newline\end{bmatrix}\cdotx=\begin{bmatrix}a \cdot x &amp; b \cdot x \newlinec \cdot x &amp; d \cdot x \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>; <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">s <span class="built_in">=</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">A * s</span><br></pre></td></tr></table></figure><h2 id="matrix-vector-multiplication">Matrix-Vector Multiplication</h2><p><span class="math">\[\begin{bmatrix}a &amp; b \newlinec &amp; d \newlinee &amp; f \newline\end{bmatrix}\cdot\begin{bmatrix}x \newliney \newline\end{bmatrix}=\begin{bmatrix}a \cdot x + b \cdot y \newlinec \cdot x + d \cdot y \newlinee \cdot x + f \cdot y \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>; <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>; <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">v <span class="built_in">=</span> [<span class="number">1</span>; <span class="number">1</span>; <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">A * v</span><br></pre></td></tr></table></figure><h2 id="matrix-matrix-multiplication">Matrix-Matrix Multiplication</h2><p><span class="math">\[\begin{bmatrix}a &amp; b \newlinec &amp; d \newlinee &amp; f \newline\end{bmatrix}\cdot\begin{bmatrix}w &amp; x \newliney &amp; z \newline\end{bmatrix}=\begin{bmatrix}a \cdot w + b \cdot y &amp; a \cdot x + b \cdot z \newlinec \cdot w + d \cdot y &amp; c \cdot x + d \cdot z \newlinee \cdot w + f \cdot y &amp; e \cdot x + f \cdot z \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>; <span class="number">3</span>, <span class="number">4</span>; <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">B <span class="built_in">=</span> [<span class="number">1</span>; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">A * B</span><br></pre></td></tr></table></figure><h2 id="matrix-multiplication-properties">Matrix Multiplication Properties</h2><p>Not Commutative: <span class="math">\(A B \neq B A\)</span> in general</p><p>Associative: <span class="math">\((A B) C = A (B C)\)</span></p><p>Identity: <span class="math">\(I\)</span> (or <span class="math">\(I_{n \times n}\)</span>)   <span class="math">\(s.t.\)</span>   <span class="math">\(A I = I A = A\)</span></p><p><span class="math">\[I_{3 \times 3}=\begin{bmatrix}1 &amp; 0 &amp; 0 \newline0 &amp; 1 &amp; 0 \newline0 &amp; 0 &amp; 1 \newline\end{bmatrix}\]</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I <span class="built_in">=</span> eye(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="inverse-and-transpose">Inverse and Transpose</h2><p>Transposition: <span class="math">\(A^T\)</span>   <span class="math">\(s.t.\)</span>   <span class="math">\(A_{ij} = (A^T)_{ji}\)</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>; <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>; <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">A<span class="string">'</span></span><br></pre></td></tr></table></figure><p>Inverse: <span class="math">\(A^{-1}\)</span>   <span class="math">\(s.t.\)</span>   <span class="math">\(A A^{-1} = A^{-1} A = I\)</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>; <span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>; <span class="number">7</span>, <span class="number">0</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">inv(A)</span><br></pre></td></tr></table></figure><blockquote><p>Matrices that don't have an inverse are <em>singular</em> or <em>degenerate</em></p></blockquote><p>Pseudoinverse: <span class="math">\(A^{+}\)</span>   <span class="math">\(s.t.\)</span>   <span class="math">\(A A^{+} A = A\)</span>,   <span class="math">\(A^{+} A A^{+} = A^{+}\)</span></p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A <span class="built_in">=</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>; <span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>; <span class="number">7</span>, <span class="number">0</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">pinv(A)</span><br></pre></td></tr></table></figure><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/xRMqw/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/xRMqw/lecture-slides</a></li><li><a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics" target="_blank" rel="noopener">https://en.wikibooks.org/wiki/LaTeX/Mathematics</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Linear Regression</title>
      <link href="/notes/ml-linear-regression/"/>
      <url>/notes/ml-linear-regression/</url>
      <content type="html"><![CDATA[<p>Notes of <a href="https://www.coursera.org/learn/machine-learning/supplement/cRa2m/model-representation" target="_blank" rel="noopener">Linear Regression</a></p><a id="more"></a><h2 id="model-representation">Model Representation</h2><table><thead><tr class="header"><th align="left"> </th><th align="left"> </th></tr></thead><tbody><tr class="odd"><td align="left">m</td><td align="left">number of training examples</td></tr><tr class="even"><td align="left">n</td><td align="left">number of features</td></tr><tr class="odd"><td align="left"><span class="math">\(x^{(i)}\)</span></td><td align="left">input variables / features</td></tr><tr class="even"><td align="left"><span class="math">\(y^{(i)}\)</span></td><td align="left">output variable / target</td></tr><tr class="odd"><td align="left"><span class="math">\((x^{(i)}, y^{(i)})\)</span></td><td align="left">training example</td></tr><tr class="even"><td align="left"><span class="math">\((x^{(i)}, y^{(i)}) \quad i=1 \dots m\)</span></td><td align="left">training set</td></tr><tr class="odd"><td align="left"><span class="math">\(h : X → Y\)</span></td><td align="left">hypothesis</td></tr></tbody></table><p><span class="math">\(h_\theta(x) = \theta^T x\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(x_{0}^{(i)} = 1\)</span></p><p>learning algorithm : training set → h</p><h2 id="cost-function">Cost Function</h2><p><span class="math">\(\displaystyle J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right) ^ 2\)</span></p><h2 id="gradient-descent">Gradient Descent</h2><blockquote><p>Goal: <span class="math">\(minimize _\theta J(\theta)\)</span></p></blockquote><h3 id="batch-gradient-descent">Batch Gradient Descent</h3><p>Repeat until convergence <br><span class="math">\(\;\;\)</span> <span class="math">\(\displaystyle \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \quad \text{for} \; j \gets 0 \dots n\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\alpha\)</span>: learning rate <br><span class="math">\(\;\)</span> <span class="math">\(\displaystyle \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \; x_j^{(i)}\)</span></p><p>(simultaneous update)</p><h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3><p>Randomly shuffle training examples</p><p>Repeat <br><span class="math">\(\;\;\)</span> <span class="math">\(\text{for} \; i \gets 1 \dots m\)</span> <br><span class="math">\(\;\;\;\;\)</span> <span class="math">\(\displaystyle \theta_j := \theta_j - \alpha \; (h_\theta(x^{(i)}) - y^{(i)}) \; x_j^{(i)} \quad \text{for} \; j \gets 0 \dots n\)</span></p><h3 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h3><ul><li>Batch gradient descent: Use all <span class="math">\(m\)</span> examples in each iteration</li><li>Stochastic gradient descent: Use 1 example in each iteration</li><li>Mini-batch gradient descent: Use <span class="math">\(b\)</span> examples in each iteration</li></ul><h3 id="online-learning">Online Learning</h3><p><a href="https://en.wikipedia.org/wiki/online_machine_learning" target="_blank" rel="noopener">Online Learning - Wikipedia</a></p><h3 id="feature-scaling-and-mean-normalization">Feature Scaling and Mean Normalization</h3><p><span class="math">\(x_i := \dfrac{x_i - \mu_i}{s_i}\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\mu_i\)</span>: average of all values for feature <span class="math">\(x_i\)</span> <br><span class="math">\(\;\;\)</span> <span class="math">\(s_i\)</span>: range of values (max - min) --- or standard deviation</p><h3 id="regularization">Regularization</h3><p><span class="math">\(\displaystyle J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right) ^ 2 + \lambda \sum_{j=1}^n \theta_j^2 \right]\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\lambda\)</span>: regularization parameter</p><blockquote><p><span class="math">\(\displaystyle \sum_{j=1}^n \theta_j^2\)</span> excludes the bias term <span class="math">\(\theta_0\)</span></p></blockquote><h3 id="vectorization">Vectorization</h3><p><span class="math">\(\displaystyle \theta := \theta - \alpha \frac{1}{m} X^T (X \theta - y)\)</span></p><h3 id="learning-rate">Learning Rate</h3><p>plot(1:iterations, <span class="math">\(J(θ)\)</span>)</p><ul><li>If <span class="math">\(\alpha\)</span> is too small: slow convergence</li><li>If <span class="math">\(\alpha\)</span> is too large: may not decrease on every iteration and thus may not converge</li></ul><blockquote><p>It has been proven that if learning rate <span class="math">\(\alpha\)</span> is sufficiently small, then <span class="math">\(J(θ)\)</span> will decrease on every iteration</p></blockquote><h2 id="normal-equation">Normal Equation</h2><p><span class="math">\(\theta = (X^T X)^{-1} X^T y\)</span></p><blockquote><p>There is no need to do feature scaling with the normal equation</p></blockquote><h3 id="derivation">Derivation</h3><p><span class="math">\(X = \begin{bmatrix} --- (x^{(1)})^T --- \newline --- (x^{(2)})^T --- \newline \vdots \newline --- (x^{(m)})^T --- \newline \end{bmatrix}\)</span></p><p><span class="math">\(y = \begin{bmatrix} y^{(1)} \newline y^{(2)} \newline \vdots \newline y^{(m)} \newline \end{bmatrix}\)</span></p><p><span class="math">\(X \theta - y = \begin{bmatrix} h_\theta(x^{(1)}) - y^{(1)} \newline h_\theta(x^{(2)}) - y^{(2)} \newline \vdots \newline h_\theta(x^{(m)}) - y^{(m)} \newline \end{bmatrix}\)</span></p><p><span class="math">\(\displaystyle \frac{1}{2m} (X \theta - y)^T (X \theta - y) = \frac{1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 = J(\theta)\)</span></p><p><span class="math">\(\nabla_\theta J(\theta) = X^T X \theta - X^T y\)</span></p><p><span class="math">\(\nabla_\theta J(\theta) = 0\)</span>   <span class="math">\(\to\)</span>   <span class="math">\(\theta = (X^T X)^{-1} X^T y\)</span></p><h3 id="noninvertibility">Noninvertibility</h3><p>If <span class="math">\(X^T X\)</span> is noninvertible, the common causes might be having:</p><ul><li>Redundant features (i.e. some features are linearly dependent)</li><li>Too many features (e.g. m ≤ n)</li></ul><p>Solutions: reduce the number of features or use regularization</p><h3 id="regularization-1">Regularization</h3><p><span class="math">\(\theta = \left( X^T X + \lambda L \right) ^ {-1} X^T y\)</span></p><p>where <br><span class="math">\(\;\;\)</span> <span class="math">\(\lambda\)</span>: regularization parameter <br><span class="math">\(\;\;\)</span> <span class="math">\(L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline \end{bmatrix}\)</span></p><h2 id="comparison-of-gradient-descent-and-normal-equation">Comparison of Gradient Descent and Normal Equation</h2><table><thead><tr class="header"><th align="left">Gradient Descent</th><th align="left">Normal Equation</th></tr></thead><tbody><tr class="odd"><td align="left">Need to choose <span class="math">\(\alpha\)</span></td><td align="left">No need to choose <span class="math">\(\alpha\)</span></td></tr><tr class="even"><td align="left">Need many iterations</td><td align="left">No need to iterate</td></tr><tr class="odd"><td align="left"><span class="math">\(\mathcal{O}(k n^2)\)</span></td><td align="left"><span class="math">\(\mathcal{O}(n^3)\)</span>, need to calculate <span class="math">\((X^T X)^{-1}\)</span></td></tr><tr class="even"><td align="left">Works well when n is large</td><td align="left">Slow if n is very large</td></tr></tbody></table><blockquote><p>In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process</p></blockquote><h2 id="combine-features">Combine Features</h2><p>For example, combine <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span> into a new feature <span class="math">\(x_3\)</span> by taking <span class="math">\(x_1 \cdot x_2\)</span></p><h2 id="polynomial-regression">Polynomial Regression</h2><blockquote><p>Hypothesis function need not be linear (a straight line) if that does not fit the data well</p></blockquote><p>For example, to make it a square root function:</p><p><span class="math">\(x_2 = \sqrt{x_1}\)</span>   <span class="math">\(s.t.\)</span>   <span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}\)</span></p><blockquote><p>If choose features this way then feature scaling becomes very important</p></blockquote><hr><ul><li><a href="https://www.coursera.org/learn/machine-learning/supplement/d5Pt1/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/d5Pt1/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/ExY6Z/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/ExY6Z/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/HDawH/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/HDawH/lecture-slides</a></li><li><a href="https://www.coursera.org/learn/machine-learning/supplement/itpOu/lecture-slides" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/supplement/itpOu/lecture-slides</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Notes of Building Hexo Blog</title>
      <link href="/notes/build-hexo-blog/"/>
      <url>/notes/build-hexo-blog/</url>
      <content type="html"><![CDATA[<p>Notes of building <a href="https://airt.github.io/notes/">https://airt.github.io/notes/</a></p><a id="more"></a><h2 id="prerequisites">Prerequisites</h2><ul><li><a href="https://nodejs.org/" target="_blank" rel="noopener">Node</a></li><li><a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a></li><li><a href="https://github.com/" target="_blank" rel="noopener">GitHub</a></li></ul><h2 id="initialize">Initialize</h2><blockquote><p><a href="https://hexo.io/docs/" target="_blank" rel="noopener">Hexo documentation</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">hexo init notes</span><br></pre></td></tr></table></figure><h2 id="configure">Configure</h2><blockquote><p><a href="https://hexo.io/docs/configuration.html" target="_blank" rel="noopener">Hexo configuration</a></p></blockquote><p>Modify url configuration in <code>_config.yml</code>:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="attr">url:</span> <span class="attr">https://yourname.github.io/notes</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/notes/</span></span><br></pre></td></tr></table></figure><h2 id="install-a-theme">Install a Theme</h2><blockquote><p><a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">NexT for Hexo</a> <a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">NexT documentation</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">version=<span class="string">'v5.1.2'</span></span><br><span class="line">git <span class="built_in">clone</span> --branch <span class="variable">$version</span> --depth 1 https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>Set theme in hexo root configuration <code>_config.yml</code>:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure><h3 id="more-themes">More Themes</h3><ul><li><a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo themes</a></li><li><a href="https://github.com/lenbo-ma/hexo-theme-vno" target="_blank" rel="noopener">Vno</a></li></ul><h2 id="install-plugins">Install Plugins</h2><ul><li><a href="https://hexo.io/plugins/" target="_blank" rel="noopener">Hexo plugins</a></li><li><a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">Hexo generator feed</a></li><li><a href="https://github.com/hexojs/hexo-generator-sitemap" target="_blank" rel="noopener">Hexo generator sitemap</a></li><li><a href="https://github.com/PaicHyperionDev/hexo-generator-search" target="_blank" rel="noopener">Hexo generator search</a></li></ul><h2 id="setup-github-repository">Setup GitHub Repository</h2><h3 id="creat-a-new-repository"><a href="https://help.github.com/articles/creating-a-new-repository/" target="_blank" rel="noopener">Creat a New Repository</a></h3><p>New repository <code>https://github.com/yourname/notes</code></p><h3 id="configure-github-pages"><a href="https://help.github.com/articles/configuring-a-publishing-source-for-github-pages/" target="_blank" rel="noopener">Configure GitHub Pages</a></h3><p>At <code>https://github.com/yourname/notes/settings</code> &gt; <code>GitHub Pages</code> , choose <code>gh-pages branch</code></p><h2 id="setup-continuous-deployment">Setup Continuous Deployment</h2><h3 id="enable-travis-ci-for-repository"><a href="https://travis-ci.com/" target="_blank" rel="noopener">Enable Travis-CI for Repository</a></h3><h3 id="configure-travis-ci-scripts"><a href="https://docs.travis-ci.com/user/customizing-the-build/" target="_blank" rel="noopener">Configure Travis-CI Scripts</a></h3><p>Add script commands in <code>.travis.yml</code>:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">script:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span></span><br></pre></td></tr></table></figure><p>where npm scripts:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"scripts"</span>: &#123;</span><br><span class="line">    <span class="attr">"build"</span>: <span class="string">"hexo generate"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="setup-travis-ci-deployment-for-github-pages"><a href="https://docs.travis-ci.com/user/deployment/pages" target="_blank" rel="noopener">Setup Travis-CI Deployment for GitHub Pages</a></h3><p>Add <a href="https://docs.travis-ci.com/user/environment-variables/" target="_blank" rel="noopener">environment variables</a> <code>GITHUB_TOKEN</code> in Travis-CI settings</p><p>And add deployment configuration in <code>.travis.yml</code>:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">  provider:</span> <span class="string">pages</span></span><br><span class="line"><span class="attr">  skip_cleanup:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  github_token:</span> <span class="string">$GITHUB_TOKEN</span></span><br><span class="line"><span class="attr">  target_branch:</span> <span class="string">gh-pages</span></span><br><span class="line"><span class="attr">  local_dir:</span> <span class="string">public</span></span><br><span class="line"><span class="attr">  on:</span></span><br><span class="line"><span class="attr">    branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><h2 id="publish">Publish</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit</span><br><span class="line">git push</span><br><span class="line">open https://yourname.github.io/notes</span><br></pre></td></tr></table></figure><h2 id="remarks">Remarks</h2><h3 id="hexo-commands">Hexo Commands</h3><blockquote><p><a href="https://hexo.io/docs/commands.html" target="_blank" rel="noopener">Hexo commands</a></p></blockquote><p>Start a local server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br><span class="line">open http://localhost:4000/</span><br></pre></td></tr></table></figure><h3 id="examples">Examples</h3><p>Source of this site: <a href="https://github.com/airt/notes" target="_blank" rel="noopener">https://github.com/airt/notes</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Initialization</title>
      <link href="/notes/initialization/"/>
      <url>/notes/initialization/</url>
      <content type="html"><![CDATA[<p>Every end has a new beginning.</p>]]></content>
      
      
    </entry>
    
  
  
</search>
